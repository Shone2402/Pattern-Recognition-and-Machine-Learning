# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F_N5og64XTB2eF95ICyC7fOIhs8zIgEc
"""

import numpy as np
import pandas as pd
import os
from sklearn.preprocessing import OneHotEncoder
from scipy.special import softmax
onehot_encoder = OneHotEncoder(sparse=False)
from sklearn.datasets import load_iris
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
import os
from sklearn.metrics import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
import math
from sklearn.metrics import accuracy_score

def extract(filename):
  with open(filename) as f:                                                 #change name to trian/trian1/train2
      array = [[float(x.strip()) for x in line.strip().split(" ")] for line in f]
  d=int(array[0][0])
  f=int(array[0][1])
  
  feat_vecs=[]
  for i in range(1,f+1):
    feat_vecs.append(array[i])
  fv=np.array(feat_vecs)
  return fv 



def gradient(X, Y, W, mu):
    
    Z = - X @ W
    N = X.shape[0]
    P = softmax(Z, axis=1)
    A=Y-P
    gd = 1/N * (X.T @ A) 
    gd=gd+2*mu*W
    return gd

def gradient_descent(X, Y, max_iter=1000, eta=0.1, mu=0.01):
    """
    Very basic gradient descent algorithm with fixed eta and mu
    """
    Y_onehot = onehot_encoder.fit_transform(Y.reshape(-1,1))
    a=X.shape[1]
    b=Y_onehot.shape[1]
    W = np.zeros((a, b))
    step = 0
    step_lst ,W_lst= [],[] 
    
 
    while step < max_iter:
        
        W -= eta * gradient(X, Y_onehot, W, mu)
        step_lst.append(step)
        W_lst.append(W)
        step=step+1
        
    df = pd.DataFrame({
        'step': step_lst, 
      
    })
    return df, W

class Multiclass:
    def fit(self, X, Y):
        self.loss_steps, self.W = gradient_descent(X, Y)

    
    def predict(self, H):
        Z = - H @ self.W
        P = softmax(Z, axis=1)
        
        return np.argmax(P, axis=1),P
    


#path of the training directories
traindir=['a','a','a','a','a']
traindir[0] = 'Isolated_Digits/1/train'
traindir[1] = 'Isolated_Digits/2/train'
traindir[2] = 'Isolated_Digits/3/train'
traindir[3] = 'Isolated_Digits/5/train'
traindir[4] = 'Isolated_Digits/o/train'
ext = ('.mfcc')

trainfv=[]
true=[]


for i in range(5):
  for files in os.listdir(traindir[i]):
      if files.endswith(ext):
        data=extract(traindir[i]+'/'+files)
        temp=[]
        for k in range(len(data)):
          for j in range(len(data[0])):
            temp.append(data[k][j])
        
        trainfv.append(temp)
        true.append(i)

avg_len=0
for i in range(len(trainfv)):
  avg_len=avg_len+len(trainfv[i])

avg_len=int(avg_len/len(trainfv))
print(avg_len)

for i in range(len(trainfv)):
  leng=len(trainfv[i])
  if(leng<avg_len):
   for n in range(leng,avg_len):
    trainfv[i].append(float(0))
  else:
   trainfv[i]=trainfv[i][:avg_len]




X=np.array(trainfv)
Y=np.array(true)

model = Multiclass()

model.fit(X, Y)

from google.colab import drive
drive.mount('/content/drive')

print(X.shape)
print(Y.shape)

devdir=['a','a','a','a','a']
devdir[0] = 'Isolated_Digits/1/dev'
devdir[1] = 'Isolated_Digits/2/dev'
devdir[2] = 'Isolated_Digits/3/dev'
devdir[3] = 'Isolated_Digits/5/dev'
devdir[4] = 'Isolated_Digits/o/dev'
ext = ('.mfcc')

testfv=[]
truth=[]

for i in range(5):
  for files in os.listdir(devdir[i]):
      if files.endswith(ext):
        data=extract(devdir[i]+'/'+files)
        temp=[]
        for k in range(len(data)):
          for j in range(len(data[0])):
            temp.append(data[k][j])
    
        testfv.append(temp)
        truth.append(i)


for i in range(len(testfv)):
  leng=len(testfv[i])
  if(leng<avg_len):
   for n in range(leng,avg_len):
    testfv[i].append(float(0))
  else:
   testfv[i]=testfv[i][:avg_len]
testfv=sc.fit_transform(testfv)

def plot_ROC(S,truth,classes):
  TPR=[]
  FPR=[]

  mx=max(max(x) for x in S)
  mn=min(min(x) for x in S)
  print(mx,mn)

  x=np.linspace(mx,mn,10000)

  for k in range(1,len(x)):
    TP,TN,FP,FN=0,0,0,0

    for i in range(len(testfv)):
        for j in range(classes):
          if(S[i][j]>=x[k]):
            if(truth[i]==(j)):
                TP=TP+1
            else:
                FP=FP+1
          else:
            if(truth[i]==(j)):
                FN=FN+1
            else:
                TN=TN+1
    TPR.append(float(TP/(TP+FN)))
    FPR.append(float(FP/(FP+TN))) 

  plt.grid()
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.plot(FPR,TPR)
  plt.show()

np.array(truth)
Xtst=np.array(testfv)

y_pred,S=model.predict(Xtst)

cm = confusion_matrix(truth, y_pred)
import seaborn as sns
cm_matrix = pd.DataFrame(data=cm, columns=['True Class:1', 'True Class:2','True Class:3','True Class:5','True Class:o'], 
                                 index=['Predict Class:1', 'Predict Class:2','Predict Class:3','Predict Class:5','Predict Class:o'])
ax = plt.axes()
sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu',ax=ax)
ax.set_title('Confusion Matrix-Logistic_Regression-MFCC Dataset')
plt.show()
print(accuracy_score(truth, y_pred))

plot_ROC(S,truth,5)

def extract2(filename):
  array=[]
  with open(filename) as f:                                                 #change name to trian/trian1/train2
        array = [[float(x.strip()) for x in line.strip().split(" ")] for line in f]
  leng=int(array[0][0])
  fv=[]
  for i in range(1,2*leng,2):
    temp=[]
    temp.append(array[0][i])
    temp.append(array[0][i+1])
    fv.append(temp)
  slopes=[]
  for i in range(len(fv)-1):
    slope=[]
    a=fv[i+1][1]-fv[i][1]
    b=fv[i+1][0]-fv[i][0]
    if(b==0):
      if(a>0):
       slope.append(90)
      else:
       slope.append(-90)
    else:
      s=math.degrees(math.atan(a/b))
      slope.append(s)
    
    slopes.append(slope)
  return slopes

dirname=['a','a','a','a','a']
dirname[0] = 'Handwriting_Data/a/dev'
dirname[1] = 'Handwriting_Data/ai/dev'
dirname[2] = 'Handwriting_Data/chA/dev'
dirname[3] = 'Handwriting_Data/dA/dev'
dirname[4] = 'Handwriting_Data/lA/dev'
traindir=['a','a','a','a','a']
traindir[0] = 'Handwriting_Data/a/train'
traindir[1] = 'Handwriting_Data/ai/train'
traindir[2] = 'Handwriting_Data/chA/train'
traindir[3] = 'Handwriting_Data/dA/train'
traindir[4] = 'Handwriting_Data/lA/train'
ext = ('.txt')

trainfv=[]
true=[]

for i in range(5):
  for files in os.listdir(traindir[i]):
      if files.endswith(ext):
        data=extract2(traindir[i]+'/'+files)
        temp=[]
        for k in range(len(data)):
          for j in range(len(data[0])):
            temp.append(data[k][j])
        
        trainfv.append(temp)
        true.append(i)

print(len(trainfv))

avg_len=0
for i in range(len(trainfv)):
  avg_len=avg_len+len(trainfv[i])

avg_len=int(avg_len/len(trainfv))
print(avg_len)

max_len=0
for i in range(len(trainfv)):
  max_len=max(len(trainfv[i]),max_len)

avg_len=max_len

for i in range(len(trainfv)):
  leng=len(trainfv[i])
  if(leng<avg_len):
   for n in range(leng,avg_len):
    trainfv[i].append(float(90))
  else:
   trainfv[i]=trainfv[i][:avg_len]

   
testfv=[]
truth=[]

for i in range(5):
  for files in os.listdir(dirname[i]):
      if files.endswith(ext):
        data=extract2(dirname[i]+'/'+files)
        temp=[]
        for k in range(len(data)):
          for j in range(len(data[0])):
            temp.append(data[k][j])
    
        testfv.append(temp)
        truth.append(i)


for i in range(len(testfv)):
  leng=len(testfv[i])
  if(leng<avg_len):
   for n in range(leng,avg_len):
    testfv[i].append(float(0))
  else:
   testfv[i]=testfv[i][:avg_len]

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
trainfv=sc.fit_transform(trainfv)
testfv=sc.fit_transform(testfv)

X=np.array(trainfv)
Y=np.array(true)

model = Multiclass()

model.fit(X, Y)
Xtst=np.array(testfv)
y_pred,S=model.predict(Xtst)

cm = confusion_matrix(truth, y_pred)
import seaborn as sns
cm_matrix = pd.DataFrame(data=cm, columns=['True Class:a', 'True Class:ai','True Class:chA','True Class:dA','True Class:lA'], 
                                 index=['Predict Class:a', 'Predict Class:ai','Predict Class:chA','Predict Class:dA','Predict Class:lA'])
ax = plt.axes()
sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu',ax=ax)
ax.set_title('Confusion Matrix-Logistic_Regression-Handwritten Dataset')
plt.show()
print(accuracy_score(truth, y_pred))

plot_ROC(S,truth,5)

def extract3(filename):
  with open(filename) as f:                                                 #change name to trian/trian1/train2
      array = [[(float(x.strip())) for x in line.strip().split(" ")] for line in f]

  
  feat_vecs=[]
  for i in range(0,36):
    feat_vecs.append(array[i])
  fv=np.array(feat_vecs)
  return fv 

dirname=['a','a','a','a','a']

dirname[0] = 'Features/coast/dev'
dirname[1] = 'Features/forest/dev'
dirname[2] = 'Features/highway/dev'
dirname[3] = 'Features/mountain/dev'
dirname[4] = 'Features/opencountry/dev'
traindir=['a','a','a','a','a']
traindir[0] = 'Features/coast/train'
               
traindir[1] = 'Features/forest/train'
traindir[2] = 'Features/highway/train'
traindir[3] = 'Features/mountain/train'
traindir[4] = 'Features/opencountry/train'

trainfv=[]
true=[]

for i in range(5):
  for files in os.listdir(traindir[i]):
    
        data=extract3(traindir[i]+'/'+files)
        
        temp=[]
        for k in range(len(data)):
          for j in range(len(data[0])):
            temp.append(data[k][j])
        
        trainfv.append(temp)
        true.append(i)

print(len(trainfv))




testfv=[]
truth=[]

for i in range(5):
  for files in os.listdir(dirname[i]):
        data=extract3(dirname[i]+'/'+files)
        temp=[]
        for k in range(len(data)):
          for j in range(len(data[0])):
            temp.append(data[k][j])
    
        testfv.append(temp)
        truth.append(i)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
trainfv=sc.fit_transform(trainfv)
testfv=sc.fit_transform(testfv)

X=np.array(trainfv)
Y=np.array(true)

model = Multiclass()

model.fit(X, Y)
Xtst=np.array(testfv)
y_pred,S=model.predict(Xtst)

cm = confusion_matrix(truth, y_pred)
import seaborn as sns
cm_matrix = pd.DataFrame(data=cm, columns=['True Class:coast', 'True Class:forest','True Class:mount','True Class:highw','True Class:openc'], 
                                 index=['Predict Class:coast', 'Predict Class:forest','Predict Class:mount','Predict Class:highw','Predict Class:openc'])
ax = plt.axes()
sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu',ax=ax)
ax.set_title('Confusion Matrix-Logistic_Regression-Image Dataset')
plt.show()
print(accuracy_score(truth, y_pred))

plot_ROC(S,truth,5)

Z,array,train=[],[],[]

with open('train.txt') as f:                                                 #change name to trian/trian1/train2
    array = [[float(x) for x in line.split(",")] for line in f]
    
for x in range(len(array)):
  Z.append(int(array[x][2]))
  train.append([array[x][0],array[x][1]])

Z_tst,array,test=[],[],[]

with open('dev.txt') as f:                                                 #change name to trian/trian1/train2
    array = [[float(x) for x in line.split(",")] for line in f]
    
for x in range(len(array)):
  Z_tst.append(int(array[x][2]))
  test.append([array[x][0],array[x][1]])

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
trainfv=sc.fit_transform(train)
testfv=sc.fit_transform(test)
true=[x-1 for x in Z]
truth=[x-1 for x in Z_tst]
print(true)
print(truth)

X=np.array(trainfv)
Y=np.array(true)

model = Multiclass()

model.fit(X, Y)
Xtst=np.array(testfv)
y_pred,S=model.predict(Xtst)
print(y_pred)

cm = confusion_matrix(truth, y_pred)
import seaborn as sns
cm_matrix = pd.DataFrame(data=cm, columns=['True Class:coast', 'True Class:forest'], 
                                 index=['Predict Class:coast', 'Predict Class:forest'])
ax = plt.axes()
sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu',ax=ax)
ax.set_title('Confusion Matrix-Logistic_Regression-Synthetic Dataset')
plt.show()
print(accuracy_score(truth, y_pred))

plot_ROC(S,truth,2)